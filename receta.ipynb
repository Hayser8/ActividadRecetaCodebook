{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35814f5a",
   "metadata": {},
   "source": [
    "## **Schemna Inicial**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc2b959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Carga librerías y tu JSON\n",
    "import json\n",
    "import pandas as pd\n",
    "import pandera as pa\n",
    "from pandera import Column, Check, DataFrameSchema\n",
    "\n",
    "with open('repos.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "date_cols = [\"created_at\", \"updated_at\", \"pushed_at\"]\n",
    "for col in date_cols:\n",
    "    df[col] = pd.to_datetime(df[col], format=\"%Y-%m-%dT%H:%M:%SZ\", errors=\"raise\")\n",
    "\n",
    "print(df[date_cols].dtypes)\n",
    "\n",
    "# 3. Define el esquema de validación con Pandera\n",
    "schema = DataFrameSchema({\n",
    "    \"id\":                 Column(int,   Check.ge(0)),\n",
    "    \"node_id\":            Column(str,   nullable=False),\n",
    "    \"name\":               Column(str,   Check.str_length(min_value=1)),\n",
    "    \"full_name\":          Column(str,   Check.str_length(min_value=1)),\n",
    "    \"private\":            Column(bool),\n",
    "    \"html_url\":           Column(str,   Check.str_matches(r\"^https?://github\\.com/\")),\n",
    "    \"description\":        Column(str,   nullable=True),\n",
    "    \"fork\":               Column(bool),\n",
    "    \"url\":                Column(str,   Check.str_matches(r\"^https?://api\\.github\\.com/\")),\n",
    "    \"created_at\":         Column(pa.DateTime),\n",
    "    \"updated_at\":         Column(pa.DateTime),\n",
    "    \"pushed_at\":          Column(pa.DateTime),\n",
    "    \"size\":               Column(int,   Check.ge(0)),\n",
    "    \"stargazers_count\":   Column(int,   Check.ge(0)),\n",
    "    \"watchers_count\":     Column(int,   Check.ge(0)),\n",
    "    \"language\":           Column(str,   nullable=True),\n",
    "    \"has_issues\":         Column(bool),\n",
    "    \"forks_count\":        Column(int,   Check.ge(0)),\n",
    "    \"open_issues_count\":  Column(int,   Check.ge(0)),\n",
    "    \"default_branch\":     Column(str,   Check.str_length(min_value=1))\n",
    "})\n",
    "\n",
    "# 4. Ejecuta la validación (lazy=True para reportar todos los errores)\n",
    "schema.validate(df, lazy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb641e6",
   "metadata": {},
   "source": [
    "## **Conversión númerica y Visualización de Nan**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be57dd53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ——————————————————————————\n",
    "# Paso 2 (completo): Conversión de **todas** las columnas numéricas y detección de errores\n",
    "# ——————————————————————————\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Detecta automáticamente todas las columnas numéricas (int y float)\n",
    "numeric_cols = df.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "print(\"Columnas numéricas a convertir:\", numeric_cols)\n",
    "\n",
    "# 2. Convertir a entero\n",
    "for col in numeric_cols:\n",
    "    df[col] = pd.to_numeric(df[col], errors=\"coerce\", downcast=\"integer\")\n",
    "\n",
    "# 3. Reporte: ¿cuántos NaN quedaron tras la conversión en cada columna?\n",
    "nan_report = df[numeric_cols].isna().sum()\n",
    "print(\"\\nValores no convertidos a numérico (NaN) por columna:\\n\")\n",
    "print(nan_report)\n",
    "\n",
    "# 4. Para cada columna con NaN,\n",
    "for col, n in nan_report.items():\n",
    "    if n > 0:\n",
    "        print(f\"\\n>>> Columna '{col}' tiene {n} valores NaN tras conversión. Ejemplos:\")\n",
    "        print(df[df[col].isna()][[\"id\", col]].head(), \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cde834d",
   "metadata": {},
   "source": [
    "## **Completitud**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6cccc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ——————————————————————————\n",
    "# Paso 3: Completitud\n",
    "# ——————————————————————————\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Conteo y porcentaje de valores nulos por columna\n",
    "missing_count = df.isna().sum()\n",
    "missing_pct   = df.isna().mean()\n",
    "completitud = pd.concat([missing_count, missing_pct], axis=1)\n",
    "completitud.columns = [\"n_missing\", \"pct_missing\"]\n",
    "print(\"Resumen de valores faltantes:\\n\")\n",
    "print(completitud.sort_values(\"pct_missing\", ascending=False))\n",
    "\n",
    "# 2. Definir un umbral de tolerancia (p. ej. 5 %)\n",
    "umbral = 0.05\n",
    "cols_altos = completitud[completitud[\"pct_missing\"] > umbral].index.tolist()\n",
    "print(f\"\\nColumnas con > {umbral*100:.0f}% faltantes:\\n\", cols_altos)\n",
    "\n",
    "\n",
    "# para 'description' usar cadena vacía:\n",
    "if \"description\" in df.columns:\n",
    "    df[\"description\"] = df[\"description\"].fillna(\"\")\n",
    "\n",
    "# Para booleans, se imputa False:\n",
    "bool_cols = df.select_dtypes(include=\"bool\").columns\n",
    "for col in bool_cols:\n",
    "    df[col] = df[col].fillna(False)\n",
    "\n",
    "# Para fechas, se imputa fecha mínima o actual:\n",
    "date_cols = [\"created_at\", \"updated_at\", \"pushed_at\"]\n",
    "for col in date_cols:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].fillna(df[col].min())\n",
    "\n",
    "# 4. Se verifica de nuevo que no queden nulos en columnas críticas\n",
    "print(\"\\nChequeo tras imputación parcial:\")\n",
    "print(df[[\"description\"] + bool_cols.tolist() + date_cols].isna().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93580e17",
   "metadata": {},
   "source": [
    "## **Paso 3b manejar la completitud**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e617329c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ——————————————————————————\n",
    "# Paso 3b: Tratar columnas con muchos nulos\n",
    "# ——————————————————————————\n",
    "\n",
    "# 1. Eliminar columnas totalmente vacías\n",
    "df = df.drop(columns=[\"mirror_url\", \"license\"])\n",
    "\n",
    "# 2. Imputar homepage y language\n",
    "df[\"homepage\"] = df[\"homepage\"].fillna(\"\")           # campo de URL vacío\n",
    "df[\"language\"] = df[\"language\"].fillna(\"Unknown\")    # placeholder para lenguajes faltantes\n",
    "\n",
    "# 3. Verificar de nuevo\n",
    "cols_check = [\"description\", \"homepage\", \"language\"]\n",
    "print(\"\\nValores faltantes tras imputación/dropping:\")\n",
    "print(df[cols_check].isna().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f976c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ——————————————————————————\n",
    "# Paso 4: Detección y eliminación de duplicados ignorando columnas anidadas\n",
    "# ——————————————————————————\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Identificar columnas con valores dict o list\n",
    "nested_cols = [\n",
    "    col\n",
    "    for col in df.columns\n",
    "    if df[col].apply(lambda x: isinstance(x, (dict, list))).any()\n",
    "]\n",
    "print(\"Columnas anidadas (se excluirán de la comparación):\", nested_cols)\n",
    "\n",
    "# 2. Crear un DataFrame temporal sin esas columnas\n",
    "df_temp = df.drop(columns=nested_cols)\n",
    "\n",
    "# 3. Contar filas duplicadas en df_temp\n",
    "dup_rows = df_temp.duplicated()\n",
    "print(f\"Número de filas duplicadas (sin columnas anidadas): {dup_rows.sum()}\")\n",
    "\n",
    "# 4. Si hay duplicados completos, ver ejemplos\n",
    "if dup_rows.sum() > 0:\n",
    "    print(\"\\nEjemplos de filas duplicadas en el original:\")\n",
    "    display(df[dup_rows].head())\n",
    "\n",
    "# 5. Eliminar duplicados completos del DataFrame original\n",
    "df = df[~dup_rows].reset_index(drop=True)\n",
    "\n",
    "# 6. Verificar duplicados en la clave primaria 'id'\n",
    "id_dup = df[\"id\"].duplicated()\n",
    "print(f\"\\nNúmero de IDs repetidos: {id_dup.sum()}\")\n",
    "\n",
    "# 7. Mostrar IDs repetidos si los hay\n",
    "if id_dup.sum() > 0:\n",
    "    print(\"\\nIDs duplicados y sus registros:\")\n",
    "    display(df[id_dup][[\"id\", \"name\", \"full_name\"]])\n",
    "\n",
    "# 8. Eliminar filas con IDs duplicados (manteniendo la primera aparición)\n",
    "df = df[~id_dup].reset_index(drop=True)\n",
    "\n",
    "# 9. Confirmar que ya no quedan duplicados\n",
    "print(f\"\\nDuplicados completos tras limpieza: {df_temp.duplicated().sum()}\")\n",
    "print(f\"IDs repetidos tras limpieza: {df['id'].duplicated().sum()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4859b802",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ——————————————————————————\n",
    "# Paso 5: Consistencia entre columnas\n",
    "# ——————————————————————————\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# 1. watchers_count vs watchers\n",
    "mask_watchers = df[\"watchers_count\"] != df[\"watchers\"]\n",
    "print(f\"watchers_count != watchers: {mask_watchers.sum()} registros\")\n",
    "if mask_watchers.any():\n",
    "    display(df.loc[mask_watchers, [\"id\", \"watchers_count\", \"watchers\"]])\n",
    "\n",
    "# 2. forks_count vs forks\n",
    "mask_forks = df[\"forks_count\"] != df[\"forks\"]\n",
    "print(f\"forks_count != forks: {mask_forks.sum()} registros\")\n",
    "if mask_forks.any():\n",
    "    display(df.loc[mask_forks, [\"id\", \"forks_count\", \"forks\"]])\n",
    "\n",
    "# 3. open_issues_count vs open_issues\n",
    "mask_issues = df[\"open_issues_count\"] != df[\"open_issues\"]\n",
    "print(f\"open_issues_count != open_issues: {mask_issues.sum()} registros\")\n",
    "if mask_issues.any():\n",
    "    display(df.loc[mask_issues, [\"id\", \"open_issues_count\", \"open_issues\"]])\n",
    "\n",
    "# 4. Fecha relaciones\n",
    "mask_pushed_before_created = df[\"pushed_at\"] < df[\"created_at\"]\n",
    "print(f\"pushed_at < created_at: {mask_pushed_before_created.sum()} registros\")\n",
    "if mask_pushed_before_created.any():\n",
    "    display(df.loc[mask_pushed_before_created, [\"id\", \"created_at\", \"pushed_at\", \"updated_at\"]])\n",
    "\n",
    "mask_updated_before_created = df[\"updated_at\"] < df[\"created_at\"]\n",
    "print(f\"updated_at < created_at: {mask_updated_before_created.sum()} registros\")\n",
    "if mask_updated_before_created.any():\n",
    "    display(df.loc[mask_updated_before_created, [\"id\", \"created_at\", \"updated_at\", \"pushed_at\"]])\n",
    "\n",
    "# 5. Validación de homepage\n",
    "mask_homepage_invalid = (df[\"homepage\"] != \"\") & (~df[\"homepage\"].str.match(r\"^https?://\"))\n",
    "print(f\"homepage no vacío pero inválido: {mask_homepage_invalid.sum()} registros\")\n",
    "if mask_homepage_invalid.any():\n",
    "    display(df.loc[mask_homepage_invalid, [\"id\", \"homepage\"]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634477f0",
   "metadata": {},
   "source": [
    "## **Corregir Consistencia**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d682b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ——————————————————————————\n",
    "# Paso 5b: Corregir anomalies de pushed_at < created_at\n",
    "# ——————————————————————————\n",
    "\n",
    "# 1. Identificar el mask de inconsistencia\n",
    "mask = df[\"pushed_at\"] < df[\"created_at\"]\n",
    "print(f\"Registros a corregir (pushed_at < created_at): {mask.sum()}\")\n",
    "\n",
    "# 2. Corregir: igualar pushed_at a created_at\n",
    "df.loc[mask, \"pushed_at\"] = df.loc[mask, \"created_at\"]\n",
    "\n",
    "# 3. Verificar que ya no haya inconsistencias\n",
    "mask2 = df[\"pushed_at\"] < df[\"created_at\"]\n",
    "print(f\"Registros inconsistentes tras corrección: {mask2.sum()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f2ef41",
   "metadata": {},
   "source": [
    "## **Puntualidad**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3ef950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ——————————————————————————\n",
    "# Paso 6: Puntualidad\n",
    "# ——————————————————————————\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "now = pd.Timestamp.now()\n",
    "df[\"age_days\"] = (now - df[\"updated_at\"]).dt.days\n",
    "\n",
    "\n",
    "print(\"Estadísticas de edad (días) de los repositorios:\\n\")\n",
    "print(df[\"age_days\"].describe())\n",
    "\n",
    "\n",
    "threshold = 3650\n",
    "stale_10y = df[df[\"age_days\"] > threshold]\n",
    "print(f\"\\nRepositorios sin actualizar en más de 10 años (> {threshold} días): {stale_10y.shape[0]}\")\n",
    "\n",
    "# 4. Mostrar algunos ejemplos\n",
    "if not stale_10y.empty:\n",
    "    display(stale_10y[[\"id\", \"full_name\", \"updated_at\", \"age_days\"]].head())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853907df",
   "metadata": {},
   "source": [
    "## **Corregir Puntualidad**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d383b664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ——————————————————————————\n",
    "# Paso 6b: Eliminar repositorios sin actualizar en más de 10 años\n",
    "# ——————————————————————————\n",
    "\n",
    "\n",
    "threshold = 3650  # días\n",
    "\n",
    "# Filtra el DataFrame solo con los repos \"puntuales\"\n",
    "df = df[df[\"age_days\"] <= threshold].reset_index(drop=True)\n",
    "\n",
    "\n",
    "stale_after = df[df[\"age_days\"] > threshold]\n",
    "print(f\"Repositorios obsoletos tras limpieza: {stale_after.shape[0]}\")\n",
    "\n",
    "\n",
    "df = df.drop(columns=[\"age_days\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6317add6",
   "metadata": {},
   "source": [
    "## **Normalización**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07e898f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ——————————————————————————\n",
    "# Paso 7 : Normalización de cadenas, excluyendo columnas anidadas\n",
    "# ——————————————————————————\n",
    "\n",
    "import re\n",
    "\n",
    "# 1. Detectar columnas anidadas (dict o list)\n",
    "nested_cols = [\n",
    "    col\n",
    "    for col in df.columns\n",
    "    if df[col].apply(lambda x: isinstance(x, (dict, list))).any()\n",
    "]\n",
    "print(\"Se excluyen de normalización:\", nested_cols)\n",
    "\n",
    "# 2. Seleccionar sólo las columnas object que NO sean anidadas\n",
    "str_cols = [\n",
    "    col for col in df.select_dtypes(include=['object']).columns\n",
    "    if col not in nested_cols\n",
    "]\n",
    "print(\"Columnas a normalizar:\", str_cols)\n",
    "\n",
    "# 3. Para cada columna de texto:\n",
    "for col in str_cols:\n",
    "    # a) Quitar espacios al inicio/fin y reducir espacios múltiples\n",
    "    df[col] = df[col].apply(\n",
    "        lambda x: re.sub(r'\\s+', ' ', x.strip()) if isinstance(x, str) else x\n",
    "    )\n",
    "\n",
    "# 4. Unificar a minúsculas campos categóricos\n",
    "for col in ['name', 'full_name', 'default_branch', 'language']:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].apply(lambda x: x.lower() if isinstance(x, str) else x)\n",
    "\n",
    "# 5. Mostrar un vistazo tras la normalización\n",
    "print(\"Ejemplo de normalización:\")\n",
    "display(df.loc[:, str_cols + ['name', 'full_name', 'language']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ce7720",
   "metadata": {},
   "source": [
    "## **Desanidar Owner**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80aee8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ——————————————————————————\n",
    "# Paso 8: “Desanidar” la columna `owner`\n",
    "# ——————————————————————————\n",
    "\n",
    "# 1. Verifica que la columna owner siga presente\n",
    "if \"owner\" not in df.columns:\n",
    "    raise KeyError(\"No encuentro la columna 'owner' en el DataFrame.\")\n",
    "\n",
    "# 2. Usa json_normalize para expandir el dict en un DataFrame aparte\n",
    "owner_df = pd.json_normalize(df[\"owner\"].dropna()).add_prefix(\"owner_\")\n",
    "\n",
    "# 3. Asegura que los índices coincidan\n",
    "owner_df.index = df.index\n",
    "\n",
    "# 4. Elimina la columna original y concatena las nuevas columnas\n",
    "df = pd.concat([df.drop(columns=[\"owner\"]), owner_df], axis=1)\n",
    "\n",
    "# 5. Comprueba el resultado\n",
    "cols_owner = [c for c in df.columns if c.startswith(\"owner_\")]\n",
    "print(\"Columnas desanidadas de owner:\", cols_owner)\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2069c603",
   "metadata": {},
   "source": [
    "## **Exportar limpio**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46741c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ——————————————————————————\n",
    "# Paso 9: Exportar DataFrame limpio\n",
    "# ——————————————————————————\n",
    "\n",
    "# 1. Exportar a CSV\n",
    "df.to_csv('repos_clean.csv', index=False, encoding='utf-8')\n",
    "print(\"→ Archivo exportado: repos_clean.csv\")\n",
    "\n",
    "# 2.  Exportar a Excel\n",
    "df.to_excel('repos_clean.xlsx', index=False)\n",
    "print(\"→ Archivo exportado: repos_clean.xlsx\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
